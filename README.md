# 🧠 LLM Pipeline & Prompt Behavior Testing  
_Exploratory Work with LangChain, Mistral, and Prompt Strategy_

## 📘 Overview
This project contains early experimentation with LLM workflows using [LangChain](https://www.langchain.com/) and Mistral. The goal was to evaluate **prompt structure, model responsiveness, and pipeline behavior** under different framing scenarios — with a focus on applied product testing and behavior modeling.

Included is an activity-based prompt log and exploration of:

- How prompt phrasing impacts model output
- The structure and flexibility of LangChain’s pipeline components
- Practical considerations when chaining LLM responses across multiple task layers

The notebook has been exported to PDF for quick review.

---

## 🧩 Key Themes
- **Prompt Engineering:** How subtle framing changes influence accuracy, tone, and behavior
- **Pipeline Prototyping:** Using LangChain to simulate sequential task logic and evaluate multi-step AI tasks
- **UX Alignment:** Testing how LLMs interpret ambiguous prompts — critical for designing smart, AI-integrated product flows

---

## 💡 Why This Matters
As LLMs become embedded into product workflows, understanding not just *what* they say, but *why* they respond a certain way is essential. This work reflects my approach to:

- Rapid experimentation  
- Behavior modeling with real-world implications  
- Translating emerging tech into measurable product impact

---

## 👤 Author

**Kache Lee**  
[GitHub](https://github.com/leekbc13) • [LinkedIn](https://linkedin.com/in/kachelee) • [Email](mailto:leekbc13@gmail.com)

---

> _“I approach LLMs the same way I approach funnel optimization: with curiosity, structure, and a constant drive to test what actually works.”_

